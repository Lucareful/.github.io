<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Python爬虫 on Luenci</title>
    <link>http://localhost:1313/categories/python%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in Python爬虫 on Luenci</description>
    <generator>Hugo -- 0.129.0</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/categories/python%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>京东全网爬虫</title>
      <link>http://localhost:1313/articles/1/01/%E4%BA%AC%E4%B8%9C%E5%85%A8%E7%BD%91%E7%88%AC%E8%99%AB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/1/01/%E4%BA%AC%E4%B8%9C%E5%85%A8%E7%BD%91%E7%88%AC%E8%99%AB/</guid>
      <description>完整代码见： https://github.com/Lucareful/JingDongSpider 写在前面： 折腾了很久的用python做爬虫项目到现在也该告一段落了，看视频学，遇到bug自己查找，代码思路不对重新写，环境不对自己配置&amp;hellip;.一路上跌跌撞撞，过程很艰苦，所幸结果为好。 代码就像一面明镜，照见我自身的不足。继续加油 需求 抓取首页的分类信息 大分类的url 中分类的url 小分类的url 抓取商品信息 商品名称 价格 评论信息 店铺 促销 选项 图片 开发环境和技术 技术选择： 由于全网爬虫</description>
    </item>
    <item>
      <title>基于Python语言的IP代理池</title>
      <link>http://localhost:1313/articles/1/01/%E5%9F%BA%E4%BA%8Epython%E8%AF%AD%E8%A8%80%E7%9A%84ip%E4%BB%A3%E7%90%86%E6%B1%A0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/1/01/%E5%9F%BA%E4%BA%8Epython%E8%AF%AD%E8%A8%80%E7%9A%84ip%E4%BB%A3%E7%90%86%E6%B1%A0/</guid>
      <description>&lt;p&gt;环境：python3.6&lt;/p&gt;
&lt;p&gt;MongoDB&lt;/p&gt;
&lt;p&gt;flask&lt;/p&gt;
&lt;p&gt;requests等第三方库&lt;/p&gt;
&lt;p&gt;完整代码见： &lt;a href=&#34;https://github.com/Lucareful/IPProxyPool&#34;&gt;https://github.com/Lucareful/IPProxyPool&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;代理池概述&#34;&gt;代理池概述&lt;/h1&gt;
&lt;h2 id=&#34;什么是代理池&#34;&gt;什么是代理池&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;代理池就是有代理IP组成的池子，它可以提供多个稳定可用的代理IP&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;为什么要实现代理池&#34;&gt;为什么要实现代理池&lt;/h2&gt;
&lt;p&gt;我们在做爬虫的时候，最常见的一种反爬虫手段就是：IP反爬；也就是当同一个IP访问这个网站的次数过多，频率过高，就会限制这个IP的访问。就是需要经常换IP；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用IP代理池是其中一个比较常用的方案&lt;/li&gt;
&lt;li&gt;免费代理都是非常不稳定的，有10%是可用就很不错了&lt;/li&gt;
&lt;li&gt;一些收费代理稳定性也不好&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目的：从一堆不稳定的代理IP中，抽取高可用代理IP，给爬虫使用&lt;/p&gt;
&lt;h1 id=&#34;代理池开发环境&#34;&gt;代理池开发环境&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;python3开发语言&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;requests：发送请求，获取页面数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lxml：使用XPATH从页面提取我们想要的书籍&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pymonge：把提取到代理IP存储到MongoDB数据库中和MongoDB数据库中读取代理IP，给爬虫使用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flask：用于提供WEB服务&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;代理池工作流程&#34;&gt;代理池工作流程&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://i.ibb.co/b7YyrM5/image-20191024195902163.png&#34; alt=&#34;image-20191024195902163&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;1.代理池工作渡程描述：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代理IP采集模块-&amp;gt;采集代理IP-&amp;gt;检测代理IP-&amp;gt;如果不可用用，直接过滤掉，如果可用，指定默认分数-&amp;gt;存入数据库中&lt;/li&gt;
&lt;li&gt;代理IP检测模块-&amp;gt;从数据库中获取所有代理IP-&amp;gt;检测代理IP-&amp;gt;如果代理IP不可用用，就把分数-1，如果分数为0从数据库中删除，否则更新数据库，如果代理IP可用，恢复为默认分值，更新数据库&lt;/li&gt;
&lt;li&gt;代理API模块-&amp;gt;从数据库中高可用的代理IP给爬虫使用；&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>妹子图爬虫（爬取妹子图图片）</title>
      <link>http://localhost:1313/articles/1/01/%E5%A6%B9%E5%AD%90%E5%9B%BE%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%A6%B9%E5%AD%90%E5%9B%BE%E5%9B%BE%E7%89%87/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/1/01/%E5%A6%B9%E5%AD%90%E5%9B%BE%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%A6%B9%E5%AD%90%E5%9B%BE%E5%9B%BE%E7%89%87/</guid>
      <description>python实现妹子图爬虫（爬取妹子网图片） 一个简单的小爬虫实现爬取妹子图网站上的图片。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 #coding=utf-8 import requests from bs4 import BeautifulSoup import os all_url = &amp;#39;http://www.mzitu.com&amp;#39; #http请求头 Hostreferer = { &amp;#39;User-Agent&amp;#39;:&amp;#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&amp;#39;, &amp;#39;Referer&amp;#39;:&amp;#39;http://www.mzitu.com&amp;#39; } Picreferer = { &amp;#39;User-Agent&amp;#39;:&amp;#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&amp;#39;, &amp;#39;Referer&amp;#39;:&amp;#39;http://i.meizitu.net&amp;#39; } #此请求头破解盗链 start_html = requests.get(all_url,headers = Hostreferer) #保存地址 path = &amp;#34;D:\\mzitu\\&amp;#34; #找寻最大页数 soup = BeautifulSoup(start_html.text,&amp;#34;html.parser&amp;#34;) page = soup.find_all(&amp;#39;a&amp;#39;,class_=&amp;#39;page-numbers&amp;#39;) max_page</description>
    </item>
  </channel>
</rss>
